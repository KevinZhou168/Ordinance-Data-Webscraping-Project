# Ordinance-Data-Webscraping-Project

Today, I attemped to scrape all the data from Florida's ordinance database. I initially wrote some cursory web scraping algorithms that treated the web page as a static page. Their output text files can be found in this repository. They are called output.txt and scraped_content.txt. However, I realized that the page is dynamic, and many items were hidden within section headers. 

As such, I then tried using Selenium to expand all of the expandable elements on the webpage. From there, I'd be able to scrape all of the text as if the page was static. However, I realized this was taking too much time, so I instead tried using Selenium to interact with the page and download a readily available excel sheet (contains all the ordinance data) from the webpage. I configured the download directories within my python notebook and wrote code to extract the text from the excel upon a successful download. However, a roadblock (a clickable couldn't be found) prevented me from downloading the excel sheet, and I wasn't able to resolve this in time.

If I had more time, I would first solve the issue of downloading the excel file. Then, I would use some form of LLM to read the extracted text and determine some time of hierarchy. From there, I can construct my tree. I can use Python dictionaries to implement this tree. Each node will be a dictionary containing the text at that node as well as its children (if any). Finally, I can make the scraping algorithm that I spoke about previously into a function. This way, I can loop over the town's of Florida and call the function on each town. I'll use Selenium to click into the ordinances of each Florida town and then run my web scraping function from there. 

Overall, this was a good learning experience, and it definitely challenged me more than my previous experience with web scraping. I am happy that I was able to overcome some of the hurdles I was faced with during the assessment, and I hope to have some time later to look over the remaining obstacles and try to solve them.
